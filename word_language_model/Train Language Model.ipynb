{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import data\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\n",
    "parser.add_argument('--data', type=str, default='./data/wikitext-2',\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--model', type=str, default='LSTM',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
    "parser.add_argument('--emsize', type=int, default=200,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhid', type=int, default=200,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--lr', type=float, default=20,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=40,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=35,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', action='store_true',\n",
    "                    help='tie the word embedding and softmax weights')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--onnx-export', type=str, default='',\n",
    "                    help='path to export the final model in onnx format')\n",
    "args = parser.parse_args(\"--cuda --epochs 6\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus(args.data)\n",
    "\n",
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, args.batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % args.log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt, lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_onnx(path, batch_size, seq_len):\n",
    "    print('The model is also exported in ONNX format at {}'.\n",
    "          format(os.path.realpath(args.onnx_export)))\n",
    "    model.eval()\n",
    "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    torch.onnx.export(model, (dummy_input, hidden), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 17.30 | loss  7.63 | ppl  2060.95\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 15.90 | loss  6.86 | ppl   948.66\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 15.82 | loss  6.49 | ppl   658.15\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 15.83 | loss  6.30 | ppl   544.54\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 15.68 | loss  6.15 | ppl   470.46\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 15.46 | loss  6.07 | ppl   433.49\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 16.04 | loss  5.95 | ppl   383.93\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 15.91 | loss  5.95 | ppl   383.46\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 15.84 | loss  5.80 | ppl   331.80\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 15.81 | loss  5.78 | ppl   324.35\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 15.73 | loss  5.66 | ppl   288.28\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 15.83 | loss  5.68 | ppl   293.23\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 16.00 | loss  5.66 | ppl   286.19\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 15.96 | loss  5.55 | ppl   256.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 50.18s | valid loss  5.55 | valid ppl   257.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 16.10 | loss  5.55 | ppl   257.75\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 16.05 | loss  5.53 | ppl   252.81\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 15.90 | loss  5.37 | ppl   214.01\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 16.09 | loss  5.38 | ppl   218.08\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 16.33 | loss  5.35 | ppl   210.54\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 16.42 | loss  5.34 | ppl   208.32\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 16.17 | loss  5.33 | ppl   205.72\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 15.85 | loss  5.39 | ppl   218.82\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 16.28 | loss  5.26 | ppl   192.03\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 16.08 | loss  5.27 | ppl   194.91\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 16.83 | loss  5.17 | ppl   176.69\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 16.85 | loss  5.21 | ppl   183.21\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 16.60 | loss  5.22 | ppl   185.10\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 14.54 | loss  5.13 | ppl   169.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 49.69s | valid loss  5.28 | valid ppl   195.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 14.56 | loss  5.19 | ppl   179.47\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 15.03 | loss  5.20 | ppl   181.87\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 15.03 | loss  5.02 | ppl   151.88\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 14.53 | loss  5.08 | ppl   160.38\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 14.03 | loss  5.06 | ppl   158.34\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 14.43 | loss  5.06 | ppl   157.68\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 15.00 | loss  5.08 | ppl   160.25\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 15.52 | loss  5.14 | ppl   170.91\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 15.51 | loss  5.02 | ppl   150.93\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 15.49 | loss  5.04 | ppl   154.82\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 15.07 | loss  4.96 | ppl   142.18\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 15.46 | loss  4.99 | ppl   147.48\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 15.45 | loss  5.01 | ppl   149.41\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 15.32 | loss  4.93 | ppl   138.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 46.78s | valid loss  5.15 | valid ppl   172.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 14.56 | loss  5.00 | ppl   148.16\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 14.52 | loss  5.02 | ppl   152.01\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 15.47 | loss  4.84 | ppl   126.68\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 15.48 | loss  4.90 | ppl   134.65\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 15.45 | loss  4.90 | ppl   133.94\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 15.53 | loss  4.89 | ppl   133.22\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 15.90 | loss  4.92 | ppl   137.60\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 15.85 | loss  4.99 | ppl   147.45\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 15.99 | loss  4.87 | ppl   130.46\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 17.10 | loss  4.90 | ppl   134.88\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 16.37 | loss  4.81 | ppl   123.27\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 16.67 | loss  4.85 | ppl   127.91\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 16.62 | loss  4.88 | ppl   131.04\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 16.22 | loss  4.80 | ppl   121.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 50.04s | valid loss  5.07 | valid ppl   159.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 16.64 | loss  4.87 | ppl   130.37\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 16.50 | loss  4.90 | ppl   133.71\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 16.58 | loss  4.71 | ppl   111.51\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 16.63 | loss  4.78 | ppl   119.30\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 16.99 | loss  4.78 | ppl   119.13\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 17.10 | loss  4.78 | ppl   119.42\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 16.97 | loss  4.81 | ppl   123.27\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 17.02 | loss  4.89 | ppl   133.34\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 16.64 | loss  4.78 | ppl   118.76\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 16.37 | loss  4.80 | ppl   121.72\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 16.22 | loss  4.71 | ppl   110.65\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 16.52 | loss  4.75 | ppl   115.05\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 17.36 | loss  4.78 | ppl   118.54\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 17.45 | loss  4.71 | ppl   110.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 52.93s | valid loss  5.04 | valid ppl   154.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | lr 20.00 | ms/batch 16.94 | loss  4.77 | ppl   117.55\n",
      "| epoch   6 |   400/ 2983 batches | lr 20.00 | ms/batch 16.52 | loss  4.80 | ppl   121.24\n",
      "| epoch   6 |   600/ 2983 batches | lr 20.00 | ms/batch 17.01 | loss  4.62 | ppl   101.65\n",
      "| epoch   6 |   800/ 2983 batches | lr 20.00 | ms/batch 16.80 | loss  4.69 | ppl   108.83\n",
      "| epoch   6 |  1000/ 2983 batches | lr 20.00 | ms/batch 16.69 | loss  4.69 | ppl   109.37\n",
      "| epoch   6 |  1200/ 2983 batches | lr 20.00 | ms/batch 16.97 | loss  4.69 | ppl   109.18\n",
      "| epoch   6 |  1400/ 2983 batches | lr 20.00 | ms/batch 17.19 | loss  4.74 | ppl   113.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |  1600/ 2983 batches | lr 20.00 | ms/batch 16.83 | loss  4.81 | ppl   122.52\n",
      "| epoch   6 |  1800/ 2983 batches | lr 20.00 | ms/batch 17.43 | loss  4.69 | ppl   108.62\n",
      "| epoch   6 |  2000/ 2983 batches | lr 20.00 | ms/batch 16.55 | loss  4.73 | ppl   112.94\n",
      "| epoch   6 |  2200/ 2983 batches | lr 20.00 | ms/batch 16.53 | loss  4.63 | ppl   102.69\n",
      "| epoch   6 |  2400/ 2983 batches | lr 20.00 | ms/batch 16.94 | loss  4.66 | ppl   106.08\n",
      "| epoch   6 |  2600/ 2983 batches | lr 20.00 | ms/batch 16.37 | loss  4.70 | ppl   109.68\n",
      "| epoch   6 |  2800/ 2983 batches | lr 20.00 | ms/batch 16.93 | loss  4.63 | ppl   102.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 53.04s | valid loss  5.03 | valid ppl   152.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.96 | test ppl   142.10\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "lr = args.lr\n",
    "best_val_loss = None\n",
    "\n",
    "# create tensorboardX summary writer\n",
    "exp_name = time.strftime('%H_%M_%S')\n",
    "writer = SummaryWriter('tb_logs/{}'.format(exp_name))\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        writer.add_scalar('valid/loss', val_loss, epoch)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n",
    "\n",
    "if len(args.onnx_export) > 0:\n",
    "    # Export the model in ONNX format.\n",
    "    export_onnx(args.onnx_export, batch_size=1, seq_len=args.bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
